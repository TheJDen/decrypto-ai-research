{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling a supervised Guesser\n",
    "\n",
    "Modelling the unsupervised guesser and creating a synthetic dataset made me realize I could attempt making a supervised Guesser as well. Although there will be a lot of tradeoffs in terms of parallelization, pretraining, and clarity, hopefully it may in turn yield a better performance.\n",
    "\n",
    "We can use word2vec once more to get semantic context features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading {limit} keys\n"
     ]
    }
   ],
   "source": [
    "import word2vec_loader as wv_loader\n",
    "\n",
    "limit = 200_000\n",
    "print(\"Loading {limit} keys\")\n",
    "google_news_wv = wv_loader.load_word2vec_keyedvectors(wv_loader.GOOGLE_NEWS_PATH_NAME, limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the synthetic dataset and make a training and test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import synthetic_datamuse as sd\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "meaning_df = pandas.read_csv(sd.MEANING_CSV_PATH)\n",
    "triggerword_df = pandas.read_csv(sd.TRIGGERWORD_CSV_PATH)\n",
    "\n",
    "# together there are about 70,000 samples\n",
    "# if we save 80% for training and 20% for testing we get a similar split as MNIST\n",
    "split_ratio = 0.8\n",
    "meaning_split_index = int(len(meaning_df) * split_ratio)\n",
    "triggerword_split_index = int(len(triggerword_df) * split_ratio)\n",
    "\n",
    "meaning_train, meaning_test = meaning_df[:meaning_split_index], meaning_df[meaning_split_index:]\n",
    "triggerword_train, triggerword_test = triggerword_df[:triggerword_split_index], triggerword_df[triggerword_split_index:]\n",
    "\n",
    "\n",
    "train_df, test_df = pandas.concat([meaning_train, triggerword_train], axis=0), pandas.concat([meaning_test, triggerword_test], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive way to encode each clue would be to put each keyword embedding followed by each clue embedding in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "       return v\n",
    "    return v / norm\n",
    "\n",
    "def features_to_tensor(features):\n",
    "    words_to_vecs = features.map(wv_loader.official_keyword_to_word).map(google_news_wv.__getitem__).map(normalize)\n",
    "    return torch.from_numpy(np.array(words_to_vecs.tolist()).transpose()).contiguous()\n",
    "\n",
    "# initialize DataLoader\n",
    "train_features, train_target = train_df.drop('code_index', axis=1), train_df['code_index']\n",
    "\n",
    "# pytorch requires this to be Sequence of input, label pairs. If we can't store in RAM will make custom Dataset class\n",
    "training_data = [(features_to_tensor(train_features.iloc[i]), train_target.iloc[i]) for i in range(len(train_features))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use the negative log probability as the loss because we already found that it is a good metric for determining guesses with the unsupervised guesser. An architecture which uses sigmoids and convolutions should help the model come up with features resembling probabilistic quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from MNIST example https://github.com/nicknochnack/PyTorchin15\n",
    "import os\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "\n",
    "class ClueClassifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Conv1d(in_channels=300, out_channels=64, kernel_size=3),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(128, 24), # 24 outputs = 4 permute 3 codes\n",
    "            torch.nn.LogSoftmax(1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "# instance, loss, optimizer\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "classifier = ClueClassifier().to(device)\n",
    "optimizer  = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "\n",
    "# maximizing log-likelihood is a decent metric for this classification - see unsupervised notebook for more\n",
    "loss_func = torch.nn.NLLLoss()\n",
    "\n",
    "# train loop\n",
    "model_path = \"model_state.pt\"\n",
    "if not os.path.exists(model_path):\n",
    "    for epoch in range(10):\n",
    "        for X, y in train_dataloader: # loop through batches\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            yhat = classifier(X)\n",
    "            loss = loss_func(yhat, y)\n",
    "\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch} loss: {loss.item()}\")\n",
    "\n",
    "    with open(model_path, 'wb') as f:\n",
    "        torch.save(classifier.state_dict(), f)\n",
    "else:\n",
    "    with open(model_path, 'rb') as f:\n",
    "        classifier.load_state_dict(torch.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.1712, -3.1706, -3.1718, -3.1832, -3.1815, -3.1901, -3.1832, -3.1753,\n",
      "         -3.1788, -3.1841, -3.1648, -3.1786, -3.1819, -3.1888, -3.1787, -3.1809,\n",
      "         -3.1755, -3.1800, -3.1751, -3.1780, -3.1790, -3.1611, -3.1837, -3.1779]],\n",
      "       grad_fn=<LogSoftmaxBackward0>)\n",
      "tensor(21) 0\n"
     ]
    }
   ],
   "source": [
    "test_features, test_target = test_df.drop('code_index', axis=1), test_df['code_index']\n",
    "\n",
    "tensor, label = (features_to_tensor(test_features.iloc[0]), test_target.iloc[0])\n",
    "output = classifier(tensor.unsqueeze(0))\n",
    "print(output)\n",
    "print(torch.argmax(output), label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each training epoch, the loss stayed around 3.17 and even raised at times. As we can see, our model is having trouble learning.\n",
    "\n",
    "We are asking it to do a lot. Not only are we asking it to come up with a maximum log probability guess, but we are implicitly asking it to learn the context of the game. Unlike the supervised guesser, our net knows nothing about how each keyword and clueword isn't related, or that the cluewords should be related to some of the keywords. Maybe there is an input format which will reflect this better.\n",
    "\n",
    "What if instead of thinking of the input as a sequence \n",
    "\n",
    "```K1 K2 K3 K4 C1 C2 C3```\n",
    "\n",
    "we thought of it as a matrix\n",
    "```   \n",
    "      C1      C2     C3\n",
    "K1   K1C1    K1C2   K1C3\n",
    "\n",
    "K2   K2C1    K2C2   K2C3\n",
    "\n",
    "K3   K3C1    K3C2   K3C3\n",
    "\n",
    "K4   K4C1    K4C2   K4C3\n",
    "\n",
    "```\n",
    "\n",
    "This encodes the input as keyword-clue pairs that the classifier may have an easier time learning associations for. The matrix is also relatively small (although we have many channels for embedding features), so it may make sense to opt for a more fully-connected architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, y = next(iter(train_dataloader))\n",
    "\n",
    "# print(X.shape)\n",
    "# out = torch.nn.Conv1d(in_channels=300, out_channels=64, kernel_size=3)(X)\n",
    "# print(out.shape)\n",
    "# out = torch.nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3)(out)\n",
    "# print(out.shape)\n",
    "# out = torch.nn.Conv1d(in_channels=128, out_channels=128, kernel_size=3)(out)\n",
    "# print(out.shape)\n",
    "# out = torch.nn.Flatten()(out)\n",
    "# print(out.shape)\n",
    "# out = torch.nn.Linear(128, 24)(out)\n",
    "# print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
