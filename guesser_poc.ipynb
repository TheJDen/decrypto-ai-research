{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Internal Representation\n",
    "\n",
    "A useful abstraction is thinking of each of the opponent's keywords as a random variable. We know the opponent has a keyword card with a certain number of fixed keywords, but not what the keywords are. However, we know they must be *a* word. So we may as well think of each of their words as having some probability of being each word possible, and refine the probability values based on context and revealed information.\n",
    "\n",
    "Somewhat less obvious is that we may do the same for our own keywords; for each word, the distribution would simply have probability 1 for the keyword, and 0 for every other word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guessing with Backtracking\n",
    "\n",
    "To make a good guess, the objective would be to maximize the probability of our guess; this gives the highest likelihood of being correct and raises our expected score through interceptions.\n",
    "\n",
    "Say we have keywords [\"GARDEN\", \"MUSKET\", \"BOWL\", \"BAT\"] and clues (\"wing\", \"leaf\", \"powder\"). We might notice right away that \"wing\" makes a lot of sense with \"BAT\", so we might choose the first code number to be index 3 and remove both from consideration. This simplifies our consideration to [\"GARDEN\", \"MUSKET\", \"BOWL\"] and clues (\"leaf\", \"powder\"), where we might identify more easily now that \"leaf\" corresponds to \"GARDEN\". Again, we might choose 0 as our next code number and remove both from consideration. This yields [\"MUSKET\", \"BOWL\"] and clues (\"powder\"). It is much easier to identify that \"powder\" probably corresponds to \"MUSKET\". We notice that guessing has somewhat of a recursive property. \n",
    "\n",
    "Let's think about how this might be represented mathematically. We might say that each keyword has a random variable associated with it\n",
    "\n",
    "> K_0, K_1, K_2, K_3\n",
    "\n",
    "We might then say the each code number has a random variable associated with it\n",
    "> C_0, C_1, C_2\n",
    "\n",
    "and each corresponding clue random variables (M is for \"message\").\n",
    "\n",
    "> M_0, M_1, M_2 \n",
    "\n",
    "Then we wish to maximize\n",
    "\n",
    "> P(M_0=\"wing\", M_1=\"leaf\", M_2=\"powder\").\n",
    "\n",
    "To do so, we take the maximum of\n",
    "\n",
    "> P(C_0=i) * P(M_0=\"wing\", M_1=\"leaf\", M_2=\"powder\" | C_0=i) from i = 0 to 3.\n",
    "\n",
    "This yields\n",
    "\n",
    "> P(C_0=i) * P(M_0=\"wing\", M_1=\"leaf\", M_2=\"powder\", C_0=i) / P(C_0=i)\n",
    "\n",
    "The P(C_0=i) terms cancel! To continue, we find\n",
    "\n",
    "> P(M_0=\"wing\", M_1=\"leaf\", M_2=\"powder\", C_0=i)\n",
    "\n",
    "> =\n",
    "\n",
    "> P(M_1=\"leaf\", M_2=\"powder\" | M_0=\"wing\", C_0=i) * P(M_0=\"wing\",  C_0=i)\n",
    "\n",
    "This is where we recall the example; when we thought about the remaining words, we didn't consider the fact that we had assigned one of the previous clues a code number. That was the utility of the reduction, that it made it more simple to think about; it follows then that the probability of the remaining M_1 and M_2 is independent of the choice of M_0 and C_0, and we may remove that condition.\n",
    "\n",
    "> P(M_1=\"leaf\", M_2=\"powder\") * P(M_0=\"wing\",  C_0=i) \n",
    "\n",
    "We see a recurrence! In its entirety we write it as \n",
    "\n",
    "> f(P(M_0, M_1, M_2)) = max(f(P(M_1, M_2=)) * P(M_0,  C_0=i)) from i = 0 to 3\n",
    "\n",
    "We may calculate\n",
    "\n",
    "> P(M_0,  C_0=i)\n",
    "\n",
    "with our random variable representations! \n",
    "\n",
    "We might be able to optimize with Dynamic Programming but let's worry about that later if it proves necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class RandomVariableTracker:\n",
    "    def __init__(self, random_vars: dict):\n",
    "        self.random_vars = random_vars\n",
    "        \n",
    "    def clue_probability(self, clue: str, var_index: int):\n",
    "        # this would need to be more refined for the general case\n",
    "        return self.random_vars[var_index][clue] # P(X_i = clue)\n",
    "        \n",
    "    def max_log_probability_guess(self, clues: tuple[str], var_indices=None):\n",
    "        if not clues:\n",
    "            return 0.0, tuple()\n",
    "        \n",
    "        var_indices = var_indices if var_indices is not None else tuple(range(len(self.random_vars))) \n",
    "        clue, *remaining_clues = clues\n",
    "        remaining_clues = tuple(remaining_clues)\n",
    "                \n",
    "        max_log_probability = -math.inf\n",
    "        best_guess = None\n",
    "        for i, var_index in enumerate(var_indices):\n",
    "            remaining_var_indices = var_indices[:i] + var_indices[i + 1:]\n",
    "            \n",
    "            clue_probability = self.clue_probability(clue, var_index)\n",
    "            max_subproblem_log_probability, guess = self.max_log_probability_guess(remaining_clues, remaining_var_indices)\n",
    "            log_probability = max_subproblem_log_probability + (math.log(clue_probability) if clue_probability != 0 else -math.inf)\n",
    "            \n",
    "            if log_probability > max_log_probability:\n",
    "                max_log_probability = log_probability\n",
    "                best_guess = (var_index,) + guess\n",
    "                \n",
    "        return max_log_probability, best_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('BUTTON', 'GASOLINE', 'THEATRE', 'AQUARIUM')\n",
      "[('BUTTON', 'GASOLINE', 'THEATRE'), ('BUTTON', 'GASOLINE', 'AQUARIUM'), ('BUTTON', 'THEATRE', 'GASOLINE'), ('BUTTON', 'THEATRE', 'AQUARIUM'), ('BUTTON', 'AQUARIUM', 'GASOLINE')]\n",
      "{1.0}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import decryptogame as dg\n",
    "from collections import defaultdict\n",
    "from itertools import permutations\n",
    "\n",
    "# let's see if our recurrence yields a good guessing heuristic!\n",
    "\n",
    "# generate random keywords\n",
    "keywords = next(dg.generators.RandomKeywordCards())[0]\n",
    "print(keywords)\n",
    "\n",
    "# initialize tracker\n",
    "random_vars = [defaultdict(float) for _ in range(len(keywords))]\n",
    "\n",
    "for random_var, keyword in zip(random_vars, keywords):\n",
    "    random_var[keyword] = 1.0\n",
    "\n",
    "tracker = RandomVariableTracker(random_vars)\n",
    "\n",
    "# try every code\n",
    "codes = list(permutations(range(len(keywords)), 3))\n",
    "clues = [tuple(keywords[code_num] for code_num in code) for code in codes]\n",
    "print(clues[:5])\n",
    "\n",
    "log_probabilities_and_guesses = [tracker.max_log_probability_guess(clue) for clue in clues]\n",
    "\n",
    "probabilities = [math.exp(log_probability) for log_probability, _ in log_probabilities_and_guesses]\n",
    "guesses = [guess for _, guess in log_probabilities_and_guesses]\n",
    "\n",
    "# check that probabilities made sense and that each guess was correct\n",
    "print(set(probabilities)) # should all be 1\n",
    "print(guesses == codes) # should be True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
