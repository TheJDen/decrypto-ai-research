{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Internal Representation\n",
    "\n",
    "A useful abstraction is thinking of each of the opponent's keywords as a random variable. We know the opponent has a keyword card with a certain number of fixed keywords, but not what the keywords are. However, we know they must be *a* word. So we may as well think of each of their words as having some probability of being each word possible, and refine the probability values based on context and revealed information.\n",
    "\n",
    "Somewhat less obvious is that we may do the same for our own keywords; for each word, the distribution would simply have probability 1 for the keyword, and 0 for every other word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guessing with Backtracking\n",
    "\n",
    "To make a good guess, the objective would be to maximize the probability of our guess; this gives the highest likelihood of being correct and raises our expected score through interceptions.\n",
    "\n",
    "Say we have keywords [\"GARDEN\", \"MUSKET\", \"BOWL\", \"BAT\"] and clues (\"wing\", \"leaf\", \"powder\"). We might notice right away that \"wing\" makes a lot of sense with \"BAT\", so we might choose the first code number to be index 3 and remove both from consideration. This simplifies our consideration to [\"GARDEN\", \"MUSKET\", \"BOWL\"] and clues (\"leaf\", \"powder\"), where we might identify more easily now that \"leaf\" corresponds to \"GARDEN\". Again, we might choose 0 as our next code number and remove both from consideration. This yields [\"MUSKET\", \"BOWL\"] and clues (\"powder\"). It is much easier to identify that \"powder\" probably corresponds to \"MUSKET\". We notice that guessing has somewhat of a recursive property. \n",
    "\n",
    "Let's think about how this might be represented mathematically. We might say that each keyword has a random variable associated with it\n",
    "\n",
    "> K_0, K_1, K_2, K_3\n",
    "\n",
    "We might then say the each code number has a random variable associated with it\n",
    "> C_0, C_1, C_2\n",
    "\n",
    "and each corresponding clue random variables (M is for \"message\").\n",
    "\n",
    "> M_0, M_1, M_2 \n",
    "\n",
    "Then we wish to maximize\n",
    "\n",
    "> P(M_0=\"wing\", M_1=\"leaf\", M_2=\"powder\").\n",
    "\n",
    "To do so, we take the maximum of\n",
    "\n",
    "> P(C_0=i) * P(M_0=\"wing\", M_1=\"leaf\", M_2=\"powder\" | C_0=i) from i = 0 to 3.\n",
    "\n",
    "This yields\n",
    "\n",
    "> P(C_0=i) * P(M_0=\"wing\", M_1=\"leaf\", M_2=\"powder\", C_0=i) / P(C_0=i)\n",
    "\n",
    "The P(C_0=i) terms cancel! To continue, we find\n",
    "\n",
    "> P(M_0=\"wing\", M_1=\"leaf\", M_2=\"powder\", C_0=i)\n",
    "\n",
    "> =\n",
    "\n",
    "> P(M_1=\"leaf\", M_2=\"powder\" | M_0=\"wing\", C_0=i) * P(M_0=\"wing\",  C_0=i)\n",
    "\n",
    "This is where we recall the example; when we thought about the remaining words, we didn't consider the fact that we had assigned one of the previous clues a code number. That was the utility of the reduction, that it made it more simple to think about; it follows then that the probability of the remaining M_1 and M_2 is independent of the choice of M_0 and C_0, and we may remove that condition.\n",
    "\n",
    "> P(M_1=\"leaf\", M_2=\"powder\") * P(M_0=\"wing\",  C_0=i) \n",
    "\n",
    "We see a recurrence! In its entirety we write it as \n",
    "\n",
    "> f(P(M_0, M_1, M_2)) = max(f(P(M_1, M_2=)) * P(M_0,  C_0=i)) from i = 0 to 3\n",
    "\n",
    "We may calculate\n",
    "\n",
    "> P(M_0,  C_0=i)\n",
    "\n",
    "with a metric of our choice and our random variable representations! \n",
    "\n",
    "For clarity I should note that although I went over probabilities here, that only makes sense for a single keyword, which is what the Guesser deals with. To make this more general, when we deal with each keyword as a random variable, we should think about the *expected* probability.\n",
    "\n",
    "We might be able to optimize with Dynamic Programming but let's worry about that later if it proves necessary.\n",
    "I am going to use log probabilities becasue some of the probabilities we are working with might get very small, and it will help preserve precision. Additionally, adding and subtracting might yield us some speed benefits. This should also help in the future should we explore this more in the information theory context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "import math\n",
    "\n",
    "class RandomVariableTracker:\n",
    "    def __init__(self, random_vars: Sequence[dict]):\n",
    "        self.random_vars = random_vars\n",
    "        \n",
    "    def expected_log_probability_given_clue(self, random_var: dict, clue: str):\n",
    "        # this would need to be more refined for the general case\n",
    "        return random_var.get(clue, -math.inf) # P(X_i = clue)\n",
    "        \n",
    "    def max_expected_log_probability_guess(self, clues: tuple[str], var_indices=None):\n",
    "        if not clues:\n",
    "            return 0.0, tuple()\n",
    "        \n",
    "        var_indices = var_indices if var_indices is not None else tuple(range(len(self.random_vars))) \n",
    "        clue, *remaining_clues = clues\n",
    "        remaining_clues = tuple(remaining_clues)\n",
    "                \n",
    "        max_log2_probability = -math.inf\n",
    "        best_guess = None\n",
    "        for i, var_index in enumerate(var_indices):\n",
    "            remaining_var_indices = var_indices[:i] + var_indices[i + 1:]\n",
    "            \n",
    "            max_subproblem_log2_probability, guess = self.max_expected_log_probability_guess(remaining_clues, remaining_var_indices)\n",
    "            log2_probability = max_subproblem_log2_probability + self.expected_log_probability_given_clue(self.random_vars[var_index], clue)\n",
    "            \n",
    "            if log2_probability > max_log2_probability:\n",
    "                max_log2_probability = log2_probability\n",
    "                best_guess = (var_index,) + guess\n",
    "                \n",
    "        return max_log2_probability, best_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('GUILLOTINE', 'MEMORY', 'BLOOD', 'GOD')\n",
      "[('GUILLOTINE', 'MEMORY', 'BLOOD'), ('GUILLOTINE', 'MEMORY', 'GOD'), ('GUILLOTINE', 'BLOOD', 'MEMORY'), ('GUILLOTINE', 'BLOOD', 'GOD'), ('GUILLOTINE', 'GOD', 'MEMORY')]\n",
      "{1.0}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import decryptogame as dg\n",
    "from itertools import permutations\n",
    "\n",
    "# let's see if our recurrence yields a good guessing heuristic!\n",
    "\n",
    "# generate random keywords\n",
    "keywords = next(dg.generators.RandomKeywordCards())[0]\n",
    "print(keywords)\n",
    "\n",
    "# initialize tracker\n",
    "random_vars = [{keyword: 0.0} for keyword in keywords] # log(1.0) == 0.0\n",
    "\n",
    "tracker = RandomVariableTracker(random_vars)\n",
    "\n",
    "# try every code\n",
    "codes = list(permutations(range(len(keywords)), 3))\n",
    "clues = [tuple(keywords[code_num] for code_num in code) for code in codes]\n",
    "print(clues[:5])\n",
    "\n",
    "log2_probabilities_and_guesses = [tracker.max_expected_log_probability_guess(clue) for clue in clues]\n",
    "\n",
    "probabilities = [math.exp(log_probability) for log_probability, _ in log2_probabilities_and_guesses]\n",
    "guesses = [guess for _, guess in log2_probabilities_and_guesses]\n",
    "\n",
    "# check that probabilities made sense and that each guess was correct\n",
    "print(set(probabilities)) # should all be 1\n",
    "print(guesses == codes) # should be True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! To make an effective Guesser beyond this prototype, I wan't to reuse the max_expected_log_probability_guess function, and try several different expected_log_probability_given_clue methods. The clue_probability will also need to be a bit more complex. Rather than start a messy inheritance hierarchy, I think it would make more sense to inject these as strategies, particularly the expected_log_probability_given_clue function. Let's refactor to make this more flexible for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('GUILLOTINE', 'MEMORY', 'BLOOD'), ('GUILLOTINE', 'MEMORY', 'GOD'), ('GUILLOTINE', 'BLOOD', 'MEMORY'), ('GUILLOTINE', 'BLOOD', 'GOD'), ('GUILLOTINE', 'GOD', 'MEMORY')]\n",
      "[(0, 1, 2), (0, 1, 3), (0, 2, 1), (0, 2, 3), (0, 3, 1), (0, 3, 2), (1, 0, 2), (1, 0, 3), (1, 2, 0), (1, 2, 3), (1, 3, 0), (1, 3, 2), (2, 0, 1), (2, 0, 3), (2, 1, 0), (2, 1, 3), (2, 3, 0), (2, 3, 1), (3, 0, 1), (3, 0, 2), (3, 1, 0), (3, 1, 2), (3, 2, 0), (3, 2, 1)]\n",
      "{1.0}\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import partial, reduce\n",
    "\n",
    "# wrap RandomVariable so log probabilities and type specs are more explicit\n",
    "\n",
    "@dataclass\n",
    "class RandomVariable:\n",
    "    log_probabilities: dict\n",
    "\n",
    "def softmax_combine(log_x, log_y):\n",
    "    return log_x + math.log1p(math.exp(log_y - log_x))\n",
    "\n",
    "def log_expectation(key_to_log_val_func, random_var: RandomVariable): # inject function, this is like log(E[f(X)])\n",
    "    if not random_var.log_probabilities:\n",
    "        return -math.inf\n",
    "    return reduce(softmax_combine,\n",
    "                    (log2_prob + key_to_log_val_func(key) for key, log2_prob in random_var.log_probabilities.items())\n",
    "                    )\n",
    "\n",
    "# wrap return for more readable and cohesive output\n",
    " \n",
    "@dataclass(frozen=True)\n",
    "class Guess:\n",
    "    log_expected_probability: float = 0.0\n",
    "    code: tuple[int] = tuple()\n",
    "    \n",
    "\n",
    "def max_log_expected_probability_guess(clue_and_keyword_to_log_prob_func, random_vars: Sequence[RandomVariable], clues: tuple[str]) -> Guess:\n",
    "    keyword_to_log_prob_given_clue_funcs = (partial(clue_and_keyword_to_log_prob_func, clue) for clue in clues)\n",
    "    log_expected_probability_given_clue_funcs = [partial(log_expectation, keyword_to_log_prob_given_clue_func) for keyword_to_log_prob_given_clue_func in keyword_to_log_prob_given_clue_funcs]\n",
    "\n",
    "    # smells like dp, could see if caching helps if this ends up being bottleneck\n",
    "    def max_log_expected_prob_guess(var_indices=tuple(range(len(random_vars))), clue_index = 0) -> Guess:\n",
    "        if clue_index == len(clues):\n",
    "            return Guess() # probability 1, empty code\n",
    "                        \n",
    "        best_guess = Guess(-math.inf) # start at probability 0, is like starting max at -inf\n",
    "\n",
    "        # try each available random variable and see which yields best match probability\n",
    "        for i, var_index in enumerate(var_indices):\n",
    "            remaining_var_indices = var_indices[:i] + var_indices[i + 1:]\n",
    "            \n",
    "            subproblem_best_guess = max_log_expected_prob_guess(remaining_var_indices, clue_index + 1)\n",
    "            log_expected_probability = subproblem_best_guess.log_expected_probability + log_expected_probability_given_clue_funcs[clue_index](random_vars[var_index])\n",
    "            \n",
    "            # update max log probability and corresponding code guess\n",
    "            if log_expected_probability > best_guess.log_expected_probability:\n",
    "                best_guess = Guess(log_expected_probability, (var_index,) + subproblem_best_guess.code)\n",
    "                \n",
    "        return best_guess\n",
    "    \n",
    "    return max_log_expected_prob_guess()\n",
    "\n",
    "# naive log-probability strategy (too specific, but works to show concept)\n",
    "def simple_log_prob_clue_and_keyword(clue, keyword):\n",
    "    return (0.0 if clue == keyword else -math.inf)\n",
    "    \n",
    "# test same behavior \n",
    "\n",
    "random_vars = [RandomVariable({keyword: 0.0}) for keyword in keywords]\n",
    "\n",
    "# try every code\n",
    "codes = list(permutations(range(len(keywords)), 3))\n",
    "clues = [tuple(keywords[code_num] for code_num in code) for code in codes]\n",
    "print(clues[:5])\n",
    "\n",
    "\n",
    "simple_guess = partial(max_log_expected_probability_guess, simple_log_prob_clue_and_keyword)\n",
    "guesses = [simple_guess(random_vars, clue) for clue in clues]\n",
    "\n",
    "probabilities = [math.exp(guess.log_expected_probability) for guess in guesses]\n",
    "guessed_codes = [guess.code for guess in guesses]\n",
    "\n",
    "# check that probabilities made sense and that each guess was correct\n",
    "print(set(probabilities)) # should all be 1\n",
    "print(guessed_codes == codes) # should be True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
