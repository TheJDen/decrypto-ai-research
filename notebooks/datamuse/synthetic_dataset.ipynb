{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating better clue datasets\n",
    "\n",
    "I don't have a labelled dataset because none of my friends want to come up with thousands of clues with me :(\n",
    "    \n",
    "To create some clues that are based on having similar meaning or based on words triggered by a codeword, we can use the [Datamuse API](https://www.datamuse.com/api/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 200000 keys\n"
     ]
    }
   ],
   "source": [
    "import decryptoai.word2vec_loader.loader as wv_loader\n",
    "\n",
    "limit = 200_000\n",
    "print(f\"Loading {limit} keys\")\n",
    "google_news_wv = wv_loader.load_word2vec_keyedvectors(limit=limit, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meaning dataset\n",
      "Loading triggerword dataset\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import decryptogame as dg\n",
    "import decryptoai.config as cfg\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "def datamuse_url(endpoint: str, words: list[str]): # can add stuff for prefix/suffix support later\n",
    "    query_str = '+'.join(words)\n",
    "    return f\"https://api.datamuse.com/{endpoint}={query_str}\"\n",
    "\n",
    "async def fetch_text_response(session, url, return_id=None):\n",
    "    # return ID let's us associate the result with a paramater\n",
    "    # this allows us to know which word the reponse text is associated with\n",
    "    # despite being called asynchronously\n",
    "    async with session.get(url) as response:\n",
    "        text = await response.text()\n",
    "        return return_id, text\n",
    "\n",
    "async def fetch_text_responses(urls, return_ids):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        api_calls = [fetch_text_response(session, *args) for args in zip(urls, return_ids)]\n",
    "        return [await response for response in asyncio.as_completed(api_calls)]\n",
    "\n",
    "def create_dataset_dict(responses):\n",
    "    meaning_dataset = {}\n",
    "    for word, response in responses:\n",
    "        response_object = json.loads(response)\n",
    "        if response_object:\n",
    "            meaning_dataset[word] = response_object\n",
    "    return meaning_dataset\n",
    "\n",
    "\n",
    "# process responses for local storage\n",
    "\n",
    "async def load_dataset_from_path(path: pathlib.Path, endpoint: str, words):\n",
    "    if not path.exists():\n",
    "        if not path.parent.exists():\n",
    "            path.parent.mkdir()\n",
    "        urls = [datamuse_url(endpoint, [word]) for word in words]\n",
    "        responses = await fetch_text_responses(urls, words)\n",
    "\n",
    "        dataset = create_dataset_dict(responses)\n",
    "\n",
    "        with open(str(path), 'w') as f:\n",
    "            json.dump(dataset, f)\n",
    "    else:\n",
    "        with open(str(path)) as f:\n",
    "            dataset = json.load(f)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "meaning_dataset_path = cfg.MEANING_JSON_PATH\n",
    "triggerword_dataset_path = cfg.TRIGGERWORD_JSON_PATH\n",
    "\n",
    "official_words = list(map(wv_loader.official_keyword_to_word, dg.official_words.english.words))\n",
    "\n",
    "print(\"Loading meaning dataset\")\n",
    "meaning_dataset = await load_dataset_from_path(meaning_dataset_path, \"words?ml\", official_words)\n",
    "\n",
    "print(\"Loading triggerword dataset\")\n",
    "triggerword_dataset = await load_dataset_from_path(triggerword_dataset_path, \"words?rel_trg\", official_words)\n",
    "\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can use the similar meaning and trigger word datasets to come up with reasonable clues that would be more of a challenge for our Guesser. That is, let's see if we can make clues that follow the rules and that I might be able to guess myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('WINTER', 'PATH', 'FESTIVAL', 'POISON')\n",
      "(0, 1, 3)\n",
      "('weather', 'street', 'drug')\n",
      "('skiing', 'graph', 'pill')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def filter_illegal_cluewords(legal_clue_func, datamuse_dataset):\n",
    "    filtered_dataset = {}\n",
    "    for keyword, info in datamuse_dataset.items():\n",
    "        legal_info = [word_info for word_info in info if legal_clue_func(keyword, word_info[\"word\"])]\n",
    "        filtered_dataset[keyword] = legal_info\n",
    "    return filtered_dataset        \n",
    "\n",
    "def clueword_from_dataset(datamuse_dataset, code_word, seed=400):\n",
    "    candidate_words = []\n",
    "    scores = []\n",
    "    if code_word not in datamuse_dataset:\n",
    "        return \"garbage\"\n",
    "    for word_info in datamuse_dataset[code_word]:\n",
    "        candidate_words.append(word_info[\"word\"])\n",
    "        scores.append(word_info[\"score\"])\n",
    "    np_scores = np.asarray(scores)\n",
    "    probabilities = np_scores / np.sum(np_scores)\n",
    "    [clue] = random.Random(seed).choices(candidate_words, probabilities)\n",
    "    return clue\n",
    "\n",
    "def clue_from_codewords(datamuse_dataset, codewords, seed=100):\n",
    "    return tuple(clueword_from_dataset(datamuse_dataset, word, seed=seed) for word in codewords)\n",
    "\n",
    "def legal(keyword, word):\n",
    "    no_inclusion = (keyword not in word) and (word not in keyword)\n",
    "    no_british = word not in [\"armour\", \"moustache\", \"theatre\", \"mustache\", \"armor\", \"theater\"]\n",
    "    return no_inclusion and no_british and word in google_news_wv \n",
    "\n",
    "def codewords(keyword_card, code):\n",
    "    return  [wv_loader.official_keyword_to_word(keyword_card[i]) for i in code]\n",
    "\n",
    "\n",
    "meaning_dataset = filter_illegal_cluewords(legal, meaning_dataset)\n",
    "triggerword_dataset = filter_illegal_cluewords(legal, triggerword_dataset)\n",
    "\n",
    "keyword_card_length = 4\n",
    "\n",
    "[test_keyword_card] = next(dg.generators.RandomKeywordCards(card_lengths=[keyword_card_length], seed=200))\n",
    "[test_code] = next(dg.generators.RandomCodes([test_keyword_card], seed=200))\n",
    "test_codewords = codewords(test_keyword_card, test_code)\n",
    "\n",
    "print(test_keyword_card)\n",
    "print(test_code)\n",
    "\n",
    "meaning_clue = clue_from_codewords(meaning_dataset, test_codewords)\n",
    "triggerword_clue = clue_from_codewords(triggerword_dataset, test_codewords)\n",
    "\n",
    "print(meaning_clue)\n",
    "print(triggerword_clue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are some reasonable clues! Let's save a csv for ease-of-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword1</th>\n",
       "      <th>keyword2</th>\n",
       "      <th>keyword3</th>\n",
       "      <th>keyword4</th>\n",
       "      <th>clue1</th>\n",
       "      <th>clue2</th>\n",
       "      <th>clue3</th>\n",
       "      <th>code_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31507</th>\n",
       "      <td>LUNCH</td>\n",
       "      <td>PHOTOGRAPH</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>CORRUPTION</td>\n",
       "      <td>damage</td>\n",
       "      <td>supper</td>\n",
       "      <td>ricochet</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15815</th>\n",
       "      <td>PLAGUE</td>\n",
       "      <td>COOKIE</td>\n",
       "      <td>DRESS</td>\n",
       "      <td>BEACH</td>\n",
       "      <td>coast</td>\n",
       "      <td>decorate</td>\n",
       "      <td>jelly</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29556</th>\n",
       "      <td>RAIN</td>\n",
       "      <td>CHILDHOOD</td>\n",
       "      <td>HORROR</td>\n",
       "      <td>ELECTION</td>\n",
       "      <td>horrible</td>\n",
       "      <td>storm</td>\n",
       "      <td>infant</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24419</th>\n",
       "      <td>PSYCHOLOGIST</td>\n",
       "      <td>CAT</td>\n",
       "      <td>LEGEND</td>\n",
       "      <td>ROOM</td>\n",
       "      <td>puke</td>\n",
       "      <td>hotel</td>\n",
       "      <td>folklore</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>MEMORY</td>\n",
       "      <td>BANANA</td>\n",
       "      <td>BARRACK</td>\n",
       "      <td>DUNGEON</td>\n",
       "      <td>exhort</td>\n",
       "      <td>hole</td>\n",
       "      <td>stupid</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           keyword1    keyword2 keyword3    keyword4     clue1     clue2  \\\n",
       "31507         LUNCH  PHOTOGRAPH   SPRING  CORRUPTION    damage    supper   \n",
       "15815        PLAGUE      COOKIE    DRESS       BEACH     coast  decorate   \n",
       "29556          RAIN   CHILDHOOD   HORROR    ELECTION  horrible     storm   \n",
       "24419  PSYCHOLOGIST         CAT   LEGEND        ROOM      puke     hotel   \n",
       "24977        MEMORY      BANANA  BARRACK     DUNGEON    exhort      hole   \n",
       "\n",
       "          clue3  code_index  \n",
       "31507  ricochet          19  \n",
       "15815     jelly          23  \n",
       "29556    infant          12  \n",
       "24419  folklore          11  \n",
       "24977    stupid          17  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "from itertools import permutations\n",
    "\n",
    "def all_possible_codes(keyword_card_length=4, clue_length=3):\n",
    "    return list(permutations(range(keyword_card_length), clue_length))\n",
    "\n",
    "\n",
    "meaning_csv_path = cfg.MEANING_CSV_PATH\n",
    "triggerword_csv_path = cfg.TRIGGERWORD_CSV_PATH\n",
    "\n",
    "if not meaning_csv_path.exists() or not triggerword_csv_path.exists():\n",
    "\n",
    "    num_keyword_cards = 1500\n",
    "    codes = all_possible_codes()\n",
    "    keyword_card_generator = dg.generators.RandomKeywordCards(card_lengths=[keyword_card_length], seed=100)\n",
    "\n",
    "    meaning_data = []\n",
    "    triggerword_data = []\n",
    "    for _, [keyword_card] in zip(range(num_keyword_cards), keyword_card_generator):\n",
    "        for i, code in enumerate(codes):\n",
    "            meaning_clue = clue_from_codewords(meaning_dataset, codewords(keyword_card, code))\n",
    "            meaning_data.append(keyword_card + meaning_clue + (i,))\n",
    "            \n",
    "            triggerword_clue = clue_from_codewords(triggerword_dataset, codewords(keyword_card, code))\n",
    "            triggerword_data.append(keyword_card + triggerword_clue + (i,))\n",
    "\n",
    "    header = [\"keyword1\", \"keyword2\", \"keyword3\", \"keyword4\", \"clue1\",  \"clue2\",  \"clue3\", \"code_index\"]\n",
    "    meaning_df = pandas.DataFrame(meaning_data, columns=header)\n",
    "    triggerword_df = pandas.DataFrame(triggerword_data, columns=header)\n",
    "                                      \n",
    "    meaning_df.to_csv(str(meaning_csv_path), index=False)\n",
    "    triggerword_df.to_csv(str(triggerword_csv_path), index=False)\n",
    "\n",
    "else:\n",
    "    meaning_df = pandas.read_csv(str(meaning_csv_path))\n",
    "\n",
    "meaning_df.sample(frac=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have tens of thousands of clues to reference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
