{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97556bc6",
   "metadata": {},
   "source": [
    "# Using Word2Vec Embedding to extend unsupervised guesser POC\n",
    "\n",
    "We can use Gensim to make a more powerful version of our unsupervised Proof-of-Concept. Let's see if we can make less of a toy version using the Google News Skip-Gram model with 300-feature embeddings (requires ~2GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea025752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading {limit} keys\n"
     ]
    }
   ],
   "source": [
    "import word2vec_loader as wv_loader\n",
    "\n",
    "limit = 200_000\n",
    "print(\"Loading {limit} keys\")\n",
    "google_news_wv = wv_loader.load_word2vec_keyedvectors(wv_loader.GOOGLE_NEWS_PATH_NAME, limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9c251",
   "metadata": {},
   "source": [
    "# Modelling a stronger Guesser\n",
    "\n",
    "Now that we have a bit of a grasp on how the Google News Word2Vec model is compatible with our Decrypto words, let's build a stronger guesser and compare some different probability schemes.\n",
    "\n",
    "We'll start with some naive strategies that simply manipulate the cosine similarity. I expect these to perform poorly for 2 reasons.\n",
    "\n",
    "One is that the cosine similarity doesn't really correspond to something probablistic, so in a way we are using it more as a heuristic. This could backfire because it doesn't really take word context or word frequency into account.\n",
    "\n",
    "Another more subtle reason is that the cosine similarity is symmetric. This implies that the probability of using a clue for a keyword is the same as the probability of using the keyword as a clue for the clue word if it was the keyword (that was a mouthful). We know from Baye's Theorem this isn't quite true, because it doesn't take the probabilities/frequencies of each individual word into account, nor the density of similar neighbors each word has in the vector space.\n",
    "\n",
    "Importantly, let's not forget to use log probabilities/heuristics due to our design choice in Guesser Proof-Of-Concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "477a36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# naive heuristics\n",
    "\n",
    "def log_square_cosine_similarity(clue, keyword):\n",
    "    similarity = google_news_wv.similarity(clue, keyword)\n",
    "    return 2 * math.log(abs(similarity))\n",
    "\n",
    "def log_normalized_cosine_similarity(clue, keyword):\n",
    "    similarity = google_news_wv.similarity(clue, keyword)\n",
    "    normalized_similiarity = (1 + similarity) / 2\n",
    "    return math.log(normalized_similiarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91f30e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading meaning dataset\n",
      "Loading triggerword dataset\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import decryptogame as dg\n",
    "import synthetic_datamuse as sd\n",
    "\n",
    "# load datasets to form clues\n",
    "\n",
    "def legal(keyword, word):\n",
    "    return word not in keyword and word in google_news_wv\n",
    "\n",
    "official_words = list(map(wv_loader.official_keyword_to_word, dg.official_words.english.words))\n",
    "\n",
    "print(\"Loading meaning dataset\")\n",
    "meaning_dataset = await sd.load_dataset_from_path(sd.MEANING_DATASET_PATH, \"words?ml\", official_words)\n",
    "meaning_dataset = sd.filter_illegal_cluewords(legal, meaning_dataset)\n",
    "\n",
    "\n",
    "print(\"Loading triggerword dataset\")\n",
    "triggerword_dataset = await sd.load_dataset_from_path(sd.TRIGGERWORD_DATASET_PATH, \"words?rel_trg\", official_words)\n",
    "triggerword_dataset = sd.filter_illegal_cluewords(legal, triggerword_dataset)\n",
    "\n",
    "\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "154b20e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_square_cosine_similarity\n",
      "meaning clue set correct guess correct: 71.25%\n",
      "triggerword clue set percent correct: 69.25%\n",
      "log_normalized_cosine_similarity\n",
      "meaning clue set correct guess correct: 72.5%\n",
      "triggerword clue set percent correct: 70.75%\n"
     ]
    }
   ],
   "source": [
    "# compare heuristics\n",
    "from dataclasses import dataclass\n",
    "from functools import partial\n",
    "from itertools import permutations\n",
    "from random_variable_guesser import Guess, RandomVariable, max_log_expected_probability_guess\n",
    "\n",
    "# create sets of tests and evaluate performance of each heuristic\n",
    "\n",
    "keyword_card_length = 4\n",
    "clue_length = 3\n",
    "\n",
    "@dataclass\n",
    "class Test:\n",
    "    clue: tuple[str]\n",
    "    code: tuple[int]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TestSet:\n",
    "    keyword_card: tuple[str]\n",
    "    cluecodepairs: list[Test]\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    test: Test\n",
    "    guess: Guess\n",
    "\n",
    "@dataclass\n",
    "class ResultSet:\n",
    "    keyword_card: tuple[str]\n",
    "    results: list[Result]\n",
    "\n",
    "\n",
    "\n",
    "def codewords(keyword_card, code):\n",
    "    return  [wv_loader.official_keyword_to_word(keyword_card[i]) for i in code]\n",
    "\n",
    "all_possible_codes = list(permutations(range(keyword_card_length), clue_length))\n",
    "\n",
    "\n",
    "def generate_clue_set(clue_from_codewords_func, keyword_card, codes=all_possible_codes):\n",
    "    tests = [Test(clue_from_codewords_func(codewords(keyword_card, code)), code) for code in codes]\n",
    "    return TestSet(keyword_card, tests)\n",
    "\n",
    "num_clue_sets = 100\n",
    "\n",
    "keyword_card_generator = dg.generators.RandomKeywordCards(card_lengths=[keyword_card_length], seed=100)\n",
    "test_keyword_cards = [keyword_card for _, [keyword_card] in zip(range(num_clue_sets), keyword_card_generator)]\n",
    "\n",
    "meaning_clue_from_code = partial(sd.clue_from_codewords, meaning_dataset)\n",
    "meaning_clue_sets = [generate_clue_set(meaning_clue_from_code, keyword_card) for keyword_card in test_keyword_cards]\n",
    "\n",
    "triggerword_clue_from_code = partial(sd.clue_from_codewords, triggerword_dataset)\n",
    "triggerword_clue_sets = [generate_clue_set(triggerword_clue_from_code, keyword_card) for keyword_card in test_keyword_cards]\n",
    "\n",
    "# len(all_possible_codes) * num_clue_sets =  total guesses for each dataset\n",
    "\n",
    "def get_result_set(strat_func, clue_set: TestSet) -> ResultSet:\n",
    "    results = []\n",
    "    random_vars = [RandomVariable({wv_loader.official_keyword_to_word(keyword): 0.0}) for keyword in clue_set.keyword_card]\n",
    "    for cluecodepair in clue_set.cluecodepairs:\n",
    "        guess = max_log_expected_probability_guess(strat_func, random_vars, cluecodepair.clue)\n",
    "        results.append(Result(cluecodepair, guess))\n",
    "    return ResultSet(clue_set.keyword_card, results)\n",
    "\n",
    "\n",
    "\n",
    "for strat_func in [log_square_cosine_similarity, log_normalized_cosine_similarity]:\n",
    "    print(strat_func.__name__)\n",
    "\n",
    "    meaning_result_sets = [get_result_set(strat_func, meaning_clue_set) for meaning_clue_set in meaning_clue_sets]\n",
    "    percent_correct = 100 * sum(result.guess.code == result.test.code for result_set in meaning_result_sets for result in result_set.results) / (len(meaning_clue_sets) * len(all_possible_codes))\n",
    "    print(f\"meaning clue set correct guess correct: {percent_correct}%\")\n",
    "\n",
    "    triggerword_result_sets = [get_result_set(strat_func, meaning_clue_set) for meaning_clue_set in triggerword_clue_sets]\n",
    "    percent_correct = 100 * sum(result.guess.code == result.test.code for result_set in triggerword_result_sets for result in result_set.results) / (len(triggerword_clue_sets) * len(all_possible_codes))\n",
    "    print(f\"triggerword clue set percent correct: {percent_correct}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17653ca7",
   "metadata": {},
   "source": [
    "Our naive guessers are performing very well! If they were to be guessing randomly, or always producing the same code, we should expect that they only get about 1 in 24 correct, or about 4.17% correct! This is far better than I was expecting; we may be able to get a lot more performance with better heuristics.\n",
    "\n",
    "A couple more probabilistic heuristics would be perhaps to take the cosine similarity as a proportion of similarity to all words in the dataset. Another might be to take the the number of words with a cosine distance from the keyword that is greater than the clue distance from the keyword as a proportion of all words in the dataset. Not only are these more probabilistic in nature, but the more subtle trait of being asymmetric in the previously described sense.\n",
    "\n",
    "Tuning these metric with parameters that depend on frequency may also be of relevance. Since we don't have that data through Gensim, [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law) may be useful.\n",
    "\n",
    "We should look into the naive guesser's performance a bit better, though. Does it have a false sense of confidence for incorrect guesses? Is it simililarly unsure about all of its guesses? Are the clues it is getting wrong reasonable to get wrong, or is there an obvious pattern that it is missing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
