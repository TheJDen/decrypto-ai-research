{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97556bc6",
   "metadata": {},
   "source": [
    "# Using Word2Vec Embedding to extend unsupervised guesser POC\n",
    "\n",
    "We can use Gensim to make a more powerful version of our unsupervised Proof-of-Concept. Let's see if we can make less of a toy version using the Google News Skip-Gram model with 300-feature embeddings (requires ~2GB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea025752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 200000 keys\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jadenrodriguez/Projects/decrypto-ai-research/word2vec_unsupervised_guessing.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jadenrodriguez/Projects/decrypto-ai-research/word2vec_unsupervised_guessing.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m limit \u001b[39m=\u001b[39m \u001b[39m200_000\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jadenrodriguez/Projects/decrypto-ai-research/word2vec_unsupervised_guessing.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading \u001b[39m\u001b[39m{\u001b[39;00mlimit\u001b[39m}\u001b[39;00m\u001b[39m keys\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jadenrodriguez/Projects/decrypto-ai-research/word2vec_unsupervised_guessing.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m google_news_wv \u001b[39m=\u001b[39m wv_loader\u001b[39m.\u001b[39;49mload_word2vec_keyedvectors(wv_loader\u001b[39m.\u001b[39;49mGOOGLE_NEWS_PATH_NAME, limit)\n",
      "File \u001b[0;32m~/Projects/decrypto-ai-research/word2vec_loader.py:16\u001b[0m, in \u001b[0;36mload_word2vec_keyedvectors\u001b[0;34m(path_str, limit)\u001b[0m\n\u001b[1;32m     13\u001b[0m     google_news_wv\u001b[39m.\u001b[39msave_word2vec_format(path_str)\n\u001b[1;32m     14\u001b[0m     \u001b[39mdel\u001b[39;00m google_news_wv\n\u001b[0;32m---> 16\u001b[0m \u001b[39mreturn\u001b[39;00m gensim\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mKeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(path_str, limit\u001b[39m=\u001b[39;49mlimit)\n",
      "File \u001b[0;32m~/Projects/decrypto-ai-research/.venv/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[1;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[1;32m   1722\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/decrypto-ai-research/.venv/lib/python3.11/site-packages/gensim/models/keyedvectors.py:2069\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2065\u001b[0m         _word2vec_read_binary(\n\u001b[1;32m   2066\u001b[0m             fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size, encoding\n\u001b[1;32m   2067\u001b[0m         )\n\u001b[1;32m   2068\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2069\u001b[0m         _word2vec_read_text(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\n\u001b[1;32m   2070\u001b[0m \u001b[39mif\u001b[39;00m kv\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(kv):\n\u001b[1;32m   2071\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   2072\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mduplicate words detected, shrinking matrix size from \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   2073\u001b[0m         kv\u001b[39m.\u001b[39mvectors\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mlen\u001b[39m(kv),\n\u001b[1;32m   2074\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/decrypto-ai-research/.venv/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1974\u001b[0m, in \u001b[0;36m_word2vec_read_text\u001b[0;34m(fin, kv, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1972\u001b[0m \u001b[39mif\u001b[39;00m line \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m   1973\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEOFError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39munexpected end of input; is count incorrect or file otherwise damaged?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1974\u001b[0m word, weights \u001b[39m=\u001b[39m _word2vec_line_to_vector(line, datatype, unicode_errors, encoding)\n\u001b[1;32m   1975\u001b[0m _add_word_to_kv(kv, counts, word, weights, vocab_size)\n",
      "File \u001b[0;32m~/Projects/decrypto-ai-research/.venv/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1979\u001b[0m, in \u001b[0;36m_word2vec_line_to_vector\u001b[0;34m(line, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_word2vec_line_to_vector\u001b[39m(line, datatype, unicode_errors, encoding):\n\u001b[0;32m-> 1979\u001b[0m     parts \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mto_unicode(line\u001b[39m.\u001b[39mrstrip(), encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39municode_errors)\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1980\u001b[0m     word, weights \u001b[39m=\u001b[39m parts[\u001b[39m0\u001b[39m], [datatype(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m parts[\u001b[39m1\u001b[39m:]]\n\u001b[1;32m   1981\u001b[0m     \u001b[39mreturn\u001b[39;00m word, weights\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import word2vec_loader as wv_loader\n",
    "\n",
    "limit = 200_000\n",
    "print(f\"Loading {limit} keys\")\n",
    "google_news_wv = wv_loader.load_word2vec_keyedvectors(wv_loader.GOOGLE_NEWS_PATH_NAME, limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9c251",
   "metadata": {},
   "source": [
    "# Modelling a stronger Guesser\n",
    "\n",
    "Now that we have a bit of a grasp on how the Google News Word2Vec model is compatible with our Decrypto words from the word2vec integration notebook, let's build a stronger guesser and compare some different probability schemes.\n",
    "\n",
    "We'll start with some naive strategies that simply manipulate the cosine similarity. I expect these to perform poorly for 2 reasons.\n",
    "\n",
    "One is that the cosine similarity doesn't really correspond to something probablistic, so in a way we are using it more as a heuristic. This could backfire because it doesn't really take word context or word frequency into account.\n",
    "\n",
    "Another more subtle reason is that the cosine similarity is symmetric. This implies that the probability of using a clue for a keyword is the same as the probability of using the keyword as a clue for the clue word if it was the keyword (that was a mouthful). We know from Baye's Theorem this isn't quite true, because it doesn't take the probabilities/frequencies of each individual word into account, nor the density of similar neighbors each word has in the vector space.\n",
    "\n",
    "Importantly, let's not forget to use log probabilities/heuristics due to our design choice in Guesser Proof-Of-Concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# naive heuristics\n",
    "\n",
    "def log_square_cosine_similarity(clue, keyword):\n",
    "    similarity = google_news_wv.similarity(clue, keyword)\n",
    "    return 2 * math.log(abs(similarity))\n",
    "\n",
    "def log_normalized_cosine_similarity(clue, keyword):\n",
    "    similarity = google_news_wv.similarity(clue, keyword)\n",
    "    normalized_similiarity = (1 + similarity) / 2\n",
    "    return math.log(normalized_similiarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b67a8d",
   "metadata": {},
   "source": [
    "See synthetic datamuse notebook if you don't have the csv files. We're going to use them to evaluate the heuristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91f30e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_square_cosine_similarity\n",
      "percent meaning clues correct: 66.25833333333334%\n",
      "percent triggerword clues correct: 77.18333333333334%\n",
      "log_normalized_cosine_similarity\n",
      "percent meaning clues correct: 68.00833333333334%\n",
      "percent triggerword clues correct: 78.11666666666666%\n"
     ]
    }
   ],
   "source": [
    "import synthetic_datamuse as sd\n",
    "import numpy as np\n",
    "import pandas\n",
    "from random_variable_guesser import Guess, RandomVariable, max_log_expected_probability_guess\n",
    "from functools import partial\n",
    "\n",
    "meaning_df = pandas.read_csv(sd.MEANING_CSV_PATH)\n",
    "triggerword_df = pandas.read_csv(sd.TRIGGERWORD_CSV_PATH)\n",
    "\n",
    "codes = sd.all_possible_codes()\n",
    "\n",
    "def get_guess(strat_func, input_row):\n",
    "    keyword_card = (input_row.keyword1, input_row.keyword2, input_row.keyword3, input_row.keyword4)\n",
    "    clue = (input_row.clue1, input_row.clue2, input_row.clue3)\n",
    "    random_vars = [RandomVariable({wv_loader.official_keyword_to_word(keyword): 0.0}) for keyword in keyword_card]\n",
    "    guess = max_log_expected_probability_guess(strat_func, random_vars, clue)\n",
    "    code_index_guess = codes.index(guess.code)\n",
    "    return pandas.Series([code_index_guess, guess.log_expected_probability], index=[\"code_index_guess\", \"log_expected_prob\"])\n",
    "\n",
    "meaning_correct = meaning_df['code_index']\n",
    "triggerword_correct = triggerword_df['code_index']\n",
    "\n",
    "for strat_func in [log_square_cosine_similarity, log_normalized_cosine_similarity]:\n",
    "    print(strat_func.__name__)\n",
    "    get_guess_with_strat = partial(get_guess, strat_func)\n",
    "\n",
    "    meaning_guesses = meaning_df.apply(get_guess_with_strat, axis=1)\n",
    "    percent_correct = 100 * np.sum(meaning_df[\"code_index\"] == meaning_guesses[\"code_index_guess\"]) / len(meaning_df)\n",
    "    print(f\"percent meaning clues correct: {percent_correct}%\")\n",
    "\n",
    "    triggerword_guesses = triggerword_df.apply(get_guess_with_strat, axis=1)\n",
    "    percent_correct = 100 * np.sum(triggerword_df[\"code_index\"] == triggerword_guesses[\"code_index_guess\"]) / len(triggerword_df)\n",
    "    print(f\"percent triggerword clues correct: {percent_correct}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17653ca7",
   "metadata": {},
   "source": [
    "Our naive guessers are performing very well! If they were to be guessing randomly, or always producing the same code, we should expect that they only get about 1 in 24 correct, or about 4.17% correct! From the time it took to evaluate, we can also conclude that a couple hundred guesses occured a second, meaning each guess takes roughly less than a hundredth of a second. This is better than I was expecting in terms of correctness; we may be able to get better correctness with better heuristics. \n",
    "\n",
    "A couple more probabilistic heuristics would be perhaps to take the cosine similarity as a proportion of similarity to all words in the dataset. Another might be to take the the number of words with a cosine distance from the keyword that is greater than the clue distance from the keyword as a proportion of all words in the dataset. Not only are these more probabilistic in nature, but the more subtle trait of being asymmetric in the previously described sense.\n",
    "\n",
    "Tuning these metrics with parameters that depend on frequency may also be of relevance. Since we don't have that data through Gensim, [Zipf's Law](https://en.wikipedia.org/wiki/Zipf%27s_law) may be useful.\n",
    "\n",
    "We should look into the naive guesser's performance a bit better, though. Does it have a false sense of confidence for incorrect guesses? Is it simililarly unsure about all of its guesses? Are the clues it is getting wrong reasonable to get wrong, or is there an obvious pattern that it is missing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
